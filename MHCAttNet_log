Mar 13

Q1. How can we fine-tune LLM to replace original pre-trained word2Vec + LSTM
Q2. Can we replace LSTM by transformer to get a better result, if so, how many epochs are required.  

Task 1. Use pre-trained PROT-BERT embedding to encode protein sequence. Train simple MLP layers (fine-tune). 

Task 2. Train light transformer from scratch.
n_head = 1 because input sequence lenght is small
num_encode_layer= 1 beacause of small amounts of training samples, when increasing the layer numbers, the results became worse
two ways to reduce dimensions (batch, seq_len, dim)

Mar 14

1. Look at more fine-tuning strategies for LLM with multimodal inputs, focus on custmozing Trainer may be? 
2. Read the Reinforcement learning paper using BLOSUM. 
3. Read that Machine Intelligence papers.


Mar 15
1. Tried different pooling stratey (with different kernel sizes) to follow the transformer
2. Tried to place Attenion layer after transformer encoder layer, result is bad
3. Already approached the results achieved by 3 layers LSTM using transformer trained from scratch (relu->mish). Still trying if we could improve (forward_dim->2046, vocab_len = pep_len/mhc_len).
4. Keep monitoring the results on fine-tuning of pre-trained Prot-Bert. 


Mar16
1. Focus on implentation of HLAB
2. Understand how to generate pseudo-sequence (NetMHCpan 4.1?)

Mar18
1. Figured out the slow training problem, the reason is in the class customized dataset, since the feature extracted is very large (batch, seq, 1024), and we can only load a batch each time due to the memeory limitation. So we have to perform feature extraction in each training epoch.
2. Moved to sentence-transformer (2019) from SBERT. Reason: allow two input, originally designed to calculate the similarity of two inputs. Tried different models with binary classification settings. 

Mar 19
1. Found out prot-bert could be used with Sentence-Transformer. Modified the SoftmaxLoss, change the (u-v) ==True--> ==False.
2. To Do list: 1. Customize the evaluation metrics, run the sentence transformer with different models including the Prot-Bert. 2. Try Cross-Encoder. 


Mar 20
1. Tried cross-encoder with Prot-Bert. It did not work at all, checked 'input_ids' and 'token_type_ids'[0,0,0,1,1,1,1,1,1,1,1,1,1], all normal. Still need investigation.  

Mar 21

Still did not figure out why none of pretrained LLM work in the fine-tune experiments. 

1. Finetune cross-model using the fine-tune strategy from sentence transformer for full data (submitted to server)
2. Finetune Prot-bert with its recommended trainer for full data (submitted to server)

Found out just feed setence1 and setence2 like ids = tokenizer(sentence1, sentence2, truncate=True...) so that the token_typ_ids will show all zeros for the sentence1 and ones for sentence2. The paddings will also be shown as all zeros. However, we have attention mask, to discriminate input information from padding. 


Mar 22

1. Got good result for fine-tuning prot-bert. The results in the second epoch outperformed the previous 3-layer Bi-LSTM on MHC-I and peptide binding (2020). Keep running for more epochs.  
2. Follow Prof. Wang's suggestion that we should split the dataset so that train and test set will not contain the same MHC-I sequence. Reason: Test generalibility of model predictions on new MHC-I.
3. Found out be careful that may be the version of random.seed is needed to be specified when call random.sample. Furthermore, realized that when you use 'set', the order of element do not keep the same with the input list. 
  
Mar 23. 
1. Finish dataset split today, while satisfyting two criteria: (1) 80/20 train/test split of unqiue MHC-I sequences (2) 80/20 train/test split of total MHC samples, which are already done and (3) List unique MHC sequnces in the train and test sets. then find the indices of the sequnces in MHC list, then form a new dataset together along with Peptide and labels.   

Mar 24
1, Run the LSTM for the new split data

Mar 25 
1. Run transformer from scratch and fine-tune prot-bert for the new split data 
2. Bo suggested creat a bert for learning peptides with unsupervised manner, then fine-tune on downstram tasks (e.g., binding.) Need to do literature review, for example, how much peptide data weh have, model size and parameters tuning, training time.

Mar 28
1. Got better results by fine-tuning Prot-Bert with 10 epochs, outperforming 3-layer-bilstm netwok by almost 3% so far: 'eval_ROC_AUC': 0.9248992937524466, 'eval_PRC_AUC': 0.9596559014600455
  

Mar 29
1. Discussed with Bo that we are going to submit a manuscript to NeurIPS. The work will be training a bert-like model using peptide sequence. Dataset:  Uniprot/sprot (569,213 seqs, manually annotated) and Uniprot/trembl (245,871,724 seqs, automatically annotated)
2. Make my previous github reps being able to be downloaded with pip command (Low Priority)


April 11
Train LLM: model: prot-trans from hugging face; tokenizer: pretrained tokenizer from prot-trans,
Reason: Only trained on Peptide is not enough, since we will fine-tune for binding prediction where two inputs are required. 

I also trained LLM only with peptide (priority)  

Masked language modeling, 15% masked. 


Trainind data: from MHCAttNet paper entire combinations of (1) peptide # unique 190607 (2) mhc # 4 out of unique 152
If using all uniques mhc, it will take 2200 hours on 4 gpu, with batchsize of 8.

 


May 26

import logging
logging.basicConfig(level=logging.INFO)
always required for print logging information (e.g, loss) while using huggingface trainer.



When training LLM on peptide only, increasing gradient accumulation size wont effect training time. However, when fine-tune the pep-mhc sequence using two combined BERT, increasing gradient accumulation size will increase the training time linearly (e.g., accumulation size x2 --> training time x2). It has also been reported that increasing gradient accumulation size would sometime hurt the model performance. So I will not use gradient accumulation in fine-tuning very LLMs. 

About the data parallism on mutiple GPUs, the ccdb recommends flax, however it has been reported using flax would delay the training speed. So, in the current stage, I simply use multiple GPUs without changing code. 
Here is the traning time in hours, for peptide sequence pretraining when different numbers of GPUs have been used
1: 2: 4 ---> 160:104:?







 



